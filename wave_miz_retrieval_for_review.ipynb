{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7e9dee-d51b-4bfb-b6a9-1182dc266ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB These are being run as a 40 h, small machine for 1 year ont he NCI supercomputer Gadi.\n",
    "# Recommend to run with a 40 h machine for headroom, to process 1 year of data. \"Small\" machine is fine though.\n",
    "# 24 h was NOT ENOUGH last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6308450-5609-44e7-9450-b47340fe67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V10+ of the retrieval - a clean slate.\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # This helps the PDF output of waveform plots\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import stats\n",
    "import csv\n",
    "from geopy.distance import geodesic\n",
    "from IPython.core.debugger import set_trace\n",
    "import logging\n",
    "import re\n",
    "import glob\n",
    "import pdb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "os.chdir('/g/data/jk72/af1544/altikamiz/altikamiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c229969-df7c-49db-8041-ced44262f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up constants\n",
    "year = 2024\n",
    "yearstring = str(year)\n",
    "version = '0_15'     # V12 has the ice edge slope limit implemented. \n",
    "                     # V13 has the reporting of the SIC at the inner miz in the csv.\n",
    "                     # V14 has the reporting of the coast lat/lon and the \"southernmost bit of sea ice\" lat/lon in the csv. \n",
    "                     # V15 also adds reporting of the lat/lon of where SIC exceeds 80% for the first (northernmost) time.\n",
    "plotting = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3c239e-995b-491b-9f18-6cddbbde16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION DEFINITIONS!\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='./log_output/'+yearstring+'_all_'+version+'.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Define a custom log function\n",
    "def log(*args, end='\\n', **kwargs):\n",
    "    message = ' '.join(map(str, args))\n",
    "    logging.info(message)\n",
    "    print(message, end=end, **kwargs)  # Optionally print to console\n",
    "\n",
    "# OK Time to start doing slope of leading edge.\n",
    "def wf2sle2d(waveform):\n",
    "     upperthresh=0.8\n",
    "     lowerthresh=0.2\n",
    "     # plt.plot(waveform)\n",
    "     wfmax=waveform.max(dim='range')\n",
    "     # Find the first index where the threshold is exceeded\n",
    "     lower_first_exc = (waveform > lowerthresh*wfmax).argmax(dim='range')\n",
    "     upper_first_exc = (waveform > upperthresh*wfmax).argmax(dim='range')\n",
    "     # need to watch for run=0. This will manifest as both lower_first_exc and upper_first_exc being the same value.\n",
    "     # fix this by modifying lower_first_exc to be one lower in this case. It'll be more correctly reported as a high slope, rather than a NaN.\n",
    "     lower_first_exc = lower_first_exc.where(upper_first_exc > lower_first_exc, lower_first_exc-1)\n",
    "\n",
    "     run=upper_first_exc-lower_first_exc\n",
    "     rise=waveform[upper_first_exc]-waveform[lower_first_exc]\n",
    "\n",
    "     slope=rise/run\n",
    "     return(slope)\n",
    "\n",
    "def great_circle_distance(lat1, lon1, lat2, lon2):\n",
    "    # Create a tuple for each coordinate (latitude, longitude)\n",
    "    coord1 = (lat1, lon1)\n",
    "    coord2 = (lat2, lon2)\n",
    "\n",
    "    # Calculate the great circle distance\n",
    "    distance = geodesic(coord1, coord2).kilometers\n",
    "\n",
    "    return distance\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def append_to_csv(file_path, data): \n",
    "    # Check if the file exists or is empty to decide whether to write the header row\n",
    "    file_exists = os.path.isfile(file_path) and os.path.getsize(file_path) > 0\n",
    "\n",
    "    # Open the CSV file in append mode (a), which allows you to add data without overwriting the existing content\n",
    "    with open(file_path, mode='a', newline='') as file:\n",
    "        # Create a CSV writer object\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if requested and the file is empty or does not exist\n",
    "        if not file_exists: #write_header and \n",
    "            header = ['first_meas_time', \n",
    "                      'swhAtMyEdge', 'latAtMyEdge', 'lonAtMyEdge',\n",
    "                      'latAtAltiKaEdge', 'lonAtAltiKaEdge',\n",
    "                      'latAtInnerMIZ', 'lonAtInnerMIZ', 'mizWidthAlongTrackFromMyEdge', 'mizWidthAlongTrackFromAltikaEdge',\n",
    "                      'ice_edge_diff_flag', 'too_many_switches_flag', 'hit_continent_flag', 'inner_miz_sd_km', \n",
    "                      'sicedge_lat_p2', 'sicedge_lat_m2', 'ice_edge_complex_flag', 'sic_at_inner_miz',\n",
    "                      'latAtHitContinentPoint', 'lonAtHitContinentPoint', 'latAtSouthernmostSeaIce', 'lonAtSouthernmostSeaIce',\n",
    "                      'latAtSICge80', 'lonAtSICge80'\n",
    "                     ]  \n",
    "            # With the sd_km variable, it's the inner MIZ location Std.Dev, in km.\n",
    "            # Here we give 5 thresholds, see when they are exceeded, and see how close these are.\n",
    "            # If it's insensitive to the choice of threshold then the Std.Dev. will be very low.\n",
    "            # The idea is to calculate the distance from the AltiKa edge to each inner MIZ, and take the SD of these 5 distances.\n",
    "                       \n",
    "            writer.writerow(header)\n",
    "\n",
    "        # Write the new data to the CSV file\n",
    "        writer.writerow(data)\n",
    "\n",
    "def format_longitude(longitude):\n",
    "    if 0 <= longitude < 180:\n",
    "        return f\"{longitude:.1f}째 E\"\n",
    "    elif 180 <= longitude <= 360:\n",
    "        return f\"{360 - longitude:.1f}째 W\"\n",
    "    else:\n",
    "        return \"Invalid longitude value\"\n",
    "\n",
    "def format_latitude(latitude):\n",
    "    if -90 <= latitude < 0:\n",
    "        return f\"{-latitude:.1f}째 S\"\n",
    "    elif 0 <= latitude <= 90:\n",
    "        return f\"{latitude:.1f}째 N\"\n",
    "    else:\n",
    "        return \"Invalid latitude value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee2fd37-305a-491d-82d6-ef92db55434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_panel_plot_with_MIZ_retrieval(ds, latmin, latmax, outfile, plotting):\n",
    "\n",
    "    try:\n",
    "        mintime=ds.time.where((ds.open_sea_ice_flag >= 1) & (ds.open_sea_ice_flag <=4) & (ds.lat > latmin) & (ds.lat < latmax), drop=True).min() # this is the earliest time with those conditions\n",
    "        maxtime=ds.time.where((ds.open_sea_ice_flag >= 1) & (ds.open_sea_ice_flag <=4) & (ds.lat > latmin) & (ds.lat < latmax), drop=True).max() # this is the latest time with those conditions\n",
    "    except ValueError:\n",
    "        log(\"Zero data in mintime or maxtime. Aborting this iteration...\", end=\"\")        \n",
    "        return None\n",
    "\n",
    "    # subtract 30 s from mintime and add 30 sec to maxtime....\n",
    "    # This creates a buffer around the times of interest.\n",
    "    mintime_min30s=mintime - np.timedelta64(40, 's')\n",
    "    maxtime_pls30s=maxtime + np.timedelta64(40, 's')\n",
    "\n",
    "\n",
    "    # now for some more panels below this. \n",
    "    data = ds['waveforms_40hz'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    lat = ds['lat'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    range_vals = ds['range']\n",
    "    peakiness = ds['peakiness_40hz'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    sle = ds['sle_40hz'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    flag = ds['open_sea_ice_flag'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    sic = ds['sic'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    # Here are the sic at +2 and -2 degs longitude offset which we will use to assess ice edge morphology\n",
    "    sicp2 = ds['sicp2'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "    sicm2 = ds['sicm2'].where((ds.time > mintime_min30s) & (ds.time < maxtime_pls30s), drop=True)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Now try the retrevals.... \n",
    "    # first find out which of mintime or maxtime have the lower latitude...\n",
    "    lat_at_min = ds['lat'].sel(time = mintime, method=\"nearest\")\n",
    "    lat_at_max = ds['lat'].sel(time = maxtime, method=\"nearest\")\n",
    "\n",
    "    altika_iceedgelat=np.max([lat_at_min.values, lat_at_max.values])\n",
    "    altika_iceedgelatplus2=altika_iceedgelat+2\n",
    "\n",
    "    # Now for inner MIZ...\n",
    "    sic_thresh = 0.15    \n",
    "    ice_edge_difference_threshold = 1.0\n",
    "\n",
    "    # be on the lookout for \"in-out-in\" - i.e., bands of sea ice.\n",
    "    # we should use sic here (resampled from the NOAA/NSIDC CDR @ 25 km). \n",
    "    sicflag_thresh=(sic >= sic_thresh).astype(int)\n",
    "    sicflag_thresh_p2=(sicp2 >= sic_thresh).astype(int)\n",
    "    sicflag_thresh_m2=(sicm2 >= sic_thresh).astype(int) \n",
    "    sicflag_thresh_rolmed_1s=((sicflag_thresh).rolling(time=41, center=True)).median()\n",
    "    sicflag_thresh_rolmed_1s_p2=((sicflag_thresh_p2).rolling(time=41, center=True)).median()\n",
    "    sicflag_thresh_rolmed_1s_m2=((sicflag_thresh_m2).rolling(time=41, center=True)).median()\n",
    "\n",
    "    # actually have another threshold for 2 seconds. This is used later.\n",
    "    sicflag_thresh_rolmed_2s=((sicflag_thresh).rolling(time=81, center=True)).median()   # This one is used for \"in out in\".\n",
    "\n",
    "    \n",
    "    # the first time sicflag_thresh_rolmed_1s goes to 1 is \"my\" ice edge\n",
    "    # Do this at +/- 2 deg lon (about 100 km) to assess the ice edge shape:\n",
    "    # Prefer to only look at \"nice\" ice edges which don't vary too much in latitude.\n",
    "    try:\n",
    "        sicedge_lat=sicflag_thresh_rolmed_1s.lat.where(sicflag_thresh_rolmed_1s > 0.5, drop=True).max()\n",
    "        sicedge_lat_p2=sicflag_thresh_rolmed_1s.lat.where(sicflag_thresh_rolmed_1s_p2 > 0.5, drop=True).max()\n",
    "        sicedge_lat_m2=sicflag_thresh_rolmed_1s.lat.where(sicflag_thresh_rolmed_1s_m2 > 0.5, drop=True).max()\n",
    "    except ValueError:\n",
    "        log(\"We have a sic ValueError. Aborting this iteration...\", end=\"\")        \n",
    "        return None\n",
    "    \n",
    "    # and reject if these are too different\n",
    "    ice_edge_diff_flag = False\n",
    "    if np.abs(sicedge_lat.values - altika_iceedgelat) > ice_edge_difference_threshold:\n",
    "        log(\"Ice edge difference between AltiKa's reported edge and that calculated from NOAA CDR sic. We will include this line in output anyway... \", end=\"\")        \n",
    "        ice_edge_diff_flag = True\n",
    "\n",
    "    ice_edge_complex_flag = False\n",
    "    ice_edge_lat_threshold = 1 # degrees\n",
    "    if (np.abs(sicedge_lat.values - sicedge_lat_p2.values) > ice_edge_lat_threshold) or (np.abs(sicedge_lat.values - sicedge_lat_m2.values) > ice_edge_lat_threshold) or (np.abs(sicedge_lat_m2.values - sicedge_lat_p2.values) > ice_edge_lat_threshold):\n",
    "        log(\"Ice edge latitude varies considerably between +/- 2 degrees longitude. We will include this line in output anyway... \", end=\"\")        \n",
    "        ice_edge_complex_flag = True\n",
    "        \n",
    "\n",
    "    smoothing_width=41                             # 41 samples corresponds to a second = ~7 km\n",
    "    product_threshold=[5400,5700,6000,6300,6600]   # Determined experimentally\n",
    "    SLE_peakiness_product=sle*peakiness \n",
    "    SLE_peakiness_smoothed=SLE_peakiness_product.rolling(time=smoothing_width, center=True).mean() \n",
    "    data_array = SLE_peakiness_smoothed.values\n",
    "    local_maxima_indices = argrelextrema(data_array, np.greater)\n",
    "    local_maxima_values = data_array[local_maxima_indices]\n",
    "    local_maxima_data = xr.DataArray(local_maxima_values, dims=['maxima_index'], coords={'maxima_index': local_maxima_indices[0]})\n",
    "    miz_inner_lat=np.zeros(5)\n",
    "    for i in range(5):\n",
    "        local_maxima_data = local_maxima_data.where(local_maxima_data > product_threshold[i]).dropna(dim='maxima_index') \n",
    "        try:\n",
    "            miz_inner_lat_onecandidate=SLE_peakiness_smoothed.lat.isel(time=local_maxima_data.maxima_index[-1]).values\n",
    "            miz_inner_lat_othercandidate=SLE_peakiness_smoothed.lat.isel(time=local_maxima_data.maxima_index[0]).values\n",
    "            miz_inner_lat[i]=np.max([miz_inner_lat_onecandidate, miz_inner_lat_othercandidate])\n",
    "        except IndexError:\n",
    "            log(\"IndexError in the MIZ retrieval. Aborting this iteration... \", end=\"\")        \n",
    "            return None\n",
    "    if 0 in miz_inner_lat:\n",
    "        log(\"Still a 0 in this MIZ candidates array... Aborting...\", end=\"\")\n",
    "        return None\n",
    "\n",
    "    # for altika's ie, we haven't actually figured out which is further north... yet\n",
    "    one_altika_ie_candidate=ds.lat.where(ds.time==mintime).mean().values\n",
    "    other_altika_ie_candidate=ds.lat.where(ds.time==maxtime).mean().values\n",
    "    lat_ie_altika=np.max([one_altika_ie_candidate, other_altika_ie_candidate])\n",
    "    \n",
    "    # subset to between ALTIKA edge and the inner MIZ. Should only have 0 or 1 transitions of myflag_thresh_rolmed_2s. 0 means it's always 1 (ice) and 1 means it goes from 0 (ocean) to 1 (ice) once. \n",
    "    # 2 or more means it must drop back to ocean.\n",
    "    inoutin_subset = sicflag_thresh_rolmed_2s.where((sicflag_thresh_rolmed_2s.lat < lat_ie_altika) & (sicflag_thresh_rolmed_2s.lat > miz_inner_lat[2]), drop=True)\n",
    "    differences = inoutin_subset.diff(dim='time')\n",
    "    num_switches = (differences != 0).sum().values\n",
    "    too_many_switches_flag = False\n",
    "    if num_switches >= 2:\n",
    "        log(\"Ice edge complex - a stretch of ocean between the ice edge and inner MIZ. We will include this line in output anyway... \", end=\"\")        \n",
    "        too_many_switches_flag = True\n",
    "\n",
    "    # If there's continent between the AltiKa edge and the inner MIZ then abort.\n",
    "    hit_continent_flag = False\n",
    "    inoutin_subset_sic = sic.where((sic.lat < lat_ie_altika) & (sic.lat > miz_inner_lat[2]), drop=True)\n",
    "    try:\n",
    "        if np.max(inoutin_subset_sic) > 1.1:\n",
    "            log(\"Hit the continent before full attenuation... \", end=\"\")        \n",
    "            hit_continent_flag = True\n",
    "    except ValueError:\n",
    "        log(\"AltiKa edge north of full attenuation... Aborting...\", end=\"\")        \n",
    "        return None\n",
    "\n",
    "    # Would also like to know the SIC at the inner edge (miz_inner_lat)\n",
    "    latitude_diff = abs(sic.lat - miz_inner_lat[2])\n",
    "    nearest_index = latitude_diff.argmin()\n",
    "    sic_at_inner_miz = sic.isel(time=nearest_index)\n",
    "\n",
    "    # Report the lat/lon at both \"when we hit the continent\" and \"the southernmost patch of sea ice\" (these can be the same... mostly the same?)\n",
    "    # For the \"southernmost patch of sea ice\", we're talking about the lat/lon on the lesser lat of \"mintime\" and \"maxtime\"\n",
    "    altika_southern_iceedge_lat=np.min([lat_at_min.values, lat_at_max.values]) \n",
    "    if altika_southern_iceedge_lat == lat_at_min.values:\n",
    "        altika_southern_iceedge_lon = ds['lon'].sel(time = mintime, method=\"nearest\").values\n",
    "    else:\n",
    "        altika_southern_iceedge_lon = ds['lon'].sel(time = maxtime, method=\"nearest\").values\n",
    "\n",
    "    # Retrieve the northernmost \"5\" on flag, which is the continent.\n",
    "    timeOfHitContinent=ds.time.where((ds.open_sea_ice_flag == 5) & (ds.lat > latmin) & (ds.lat < latmax), drop=True).max() \n",
    "    otherTimeOfHitContinent=ds.time.where((ds.open_sea_ice_flag == 5) & (ds.lat > latmin) & (ds.lat < latmax), drop=True).min() \n",
    "\n",
    "    # Figure out which of these is further north\n",
    "    latAtOneTimeCont = ds['lat'].sel(time = timeOfHitContinent, method=\"nearest\").values\n",
    "    latAtOtherTimeCont = ds['lat'].sel(time = otherTimeOfHitContinent, method=\"nearest\").values\n",
    "    lonAtOneTimeCont = ds['lon'].sel(time = timeOfHitContinent, method=\"nearest\").values\n",
    "    lonAtOtherTimeCont = ds['lon'].sel(time = otherTimeOfHitContinent, method=\"nearest\").values\n",
    "\n",
    "    if latAtOneTimeCont > latAtOtherTimeCont:\n",
    "        lat_at_cont = latAtOneTimeCont\n",
    "        lon_at_cont = lonAtOneTimeCont\n",
    "    else:\n",
    "        lat_at_cont = latAtOtherTimeCont\n",
    "        lon_at_cont = lonAtOtherTimeCont\n",
    "\n",
    "\n",
    "    # Need to output the lat/lon of the northernmost point where we exceed 80%.\n",
    "    # Make sure the transition past the 80% line is calculated properly....\n",
    "    # Step 1: Sort the DataArray by latitude in descending order\n",
    "    sorted_sic = sic.sortby('lat', ascending=False)\n",
    "    # Step 2: Iterate through the sorted array to find the first occurrence where SIC exceeds 0.8\n",
    "    for i in range(len(sorted_sic)):\n",
    "        if sorted_sic[i] > 0.8:\n",
    "            first_exceedance_latitude = sorted_sic.lat[i].item()\n",
    "            time_index = ds['lat'].sel(time=slice(None)).values == first_exceedance_latitude\n",
    "            nearest_index = (abs(ds['lat'] - first_exceedance_latitude)).argmin(dim='time')\n",
    "            first_exceedance_longitude = ds['lon'][nearest_index].values\n",
    "            break\n",
    "    else:\n",
    "        first_exceedance_latitude = None  # Handle case where no value exceeds 0.8\n",
    "        first_exceedance_longitude = None\n",
    "        \n",
    "\n",
    "    #########################################\n",
    "    # Append to the stats file...\n",
    "    trackid=ds.attrs['first_meas_time']\n",
    "    lat_ie_p00=sicedge_lat.values    # this is \"my\" ice edge (i.e., I calculate it from NOAA CDR). \"p00\" = plus 0.0 degrees\n",
    "    lon_ie_p00=ds.lon.where((ds.lat>=(lat_ie_p00-0.1)) & (ds.lat<=(lat_ie_p00+0.1))).mean().values\n",
    "    swh_ie_p00=ds.swh_40hz.where((ds.lat>=(lat_ie_p00-0.1)) & (ds.lat<=(lat_ie_p00+0.1))).mean().values \n",
    "    \n",
    "    lon_ie_altika=ds.lon.where(ds.lat==lat_ie_altika).mean().values\n",
    "    lat_innermiz=miz_inner_lat\n",
    "    lon_innermiz=np.zeros(5)  \n",
    "    for i in range(5):  # retrieve the lons corresponding to the 5 lats.\n",
    "        lon_innermiz[i]=ds.lon.where((ds.lat>=(miz_inner_lat[i]-0.1)) & (ds.lat<=(miz_inner_lat[i]+0.1))).mean().values\n",
    "        \n",
    "    # Calculate the MIZ widths from \"my\" ice edge and the AltiKa ice edge.\n",
    "    mizwidth_myedge=great_circle_distance(lat_innermiz[2], lon_innermiz[2], lat_ie_p00, lon_ie_p00)\n",
    "    mizwidth_altikaedge=great_circle_distance(lat_innermiz[2], lon_innermiz[2], lat_ie_altika, lon_ie_altika)\n",
    "\n",
    "    # And to get the SD in MIZ width...\n",
    "    mizwidth_ens=np.zeros(5)\n",
    "    for i in range(5):\n",
    "        mizwidth_ens[i]=great_circle_distance(lat_innermiz[i], lon_innermiz[i], lat_ie_altika, lon_ie_altika)\n",
    "    mizwidth_SD=np.std(mizwidth_ens)\n",
    "    \n",
    "    outdata = [trackid, swh_ie_p00, lat_ie_p00, lon_ie_p00, \n",
    "               lat_ie_altika, lon_ie_altika, lat_innermiz[2], lon_innermiz[2], mizwidth_myedge, mizwidth_altikaedge,\n",
    "              int(ice_edge_diff_flag), int(too_many_switches_flag), int(hit_continent_flag), mizwidth_SD, \n",
    "              sicedge_lat_p2.values, sicedge_lat_m2.values, ice_edge_complex_flag, sic_at_inner_miz.values,\n",
    "              lat_at_cont, lon_at_cont, altika_southern_iceedge_lat, altika_southern_iceedge_lon,\n",
    "              first_exceedance_latitude, first_exceedance_longitude\n",
    "              ]\n",
    "\n",
    "    append_to_csv(outfile, outdata)\n",
    "\n",
    "    \n",
    "    # and plot if we're plotting... (slow)\n",
    "    if(plotting):\n",
    "        \n",
    "        # Set the overall size of the figure\n",
    "        fig = plt.figure(figsize=(30, 20))\n",
    "        # Create two subplots using gridspec_kw\n",
    "        gs = fig.add_gridspec(5, 1, height_ratios=[3, 1, 1, 1, 1])\n",
    "\n",
    "        ######################### \n",
    "        # Create the first subplot\n",
    "        ax1 = fig.add_subplot(gs[0])\n",
    "        # Set the size of the first subplot\n",
    "        ax1.set_aspect('auto')  # Adjust the aspect ratio if needed\n",
    "\n",
    "        ax1.set_xlabel('Latitude, Longitude')\n",
    "        ax1.set_ylabel('Range')\n",
    "        ax1.set_title('first_meas_time: '+ds.attrs['first_meas_time'])\n",
    "        # Plot data in the first subplot\n",
    "       p1=ax1.pcolormesh(lat.isel(time=slice(None, None, 8)), range_vals, data.isel(time=slice(None, None, 8)))  # factor of 8 subsample really speeds things up.\n",
    "        \n",
    "        #add longitude to the xticks\n",
    "        t,t2=plt.xticks()\n",
    "        corresponding_lons=data.swap_dims({'time':'lat'}).sel(lat=t, method='nearest').lon.values\n",
    "        for i in range(7):\n",
    "            text=t2[i].get_text()\n",
    "            modified_text = text+' N, {:.2f} E'.format(corresponding_lons[i])\n",
    "            # now modify t2 with this new text..\n",
    "            t2[i].set_text(modified_text)\n",
    "        plt.xticks(ticks=t,labels=t2)\n",
    "\n",
    "        # Add in the ice edge (from flag) and the continent edge as vertical bars.\n",
    "        plt.axvline(ds.lat.sel(time=mintime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(ds.lat.sel(time=maxtime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(sicedge_lat, color='orange', linestyle='--')\n",
    "        plt.axvline(miz_inner_lat[2], color='pink', linestyle='--')\n",
    "        plt.xlim(lat.min(), lat.max())\n",
    "\n",
    "        #########################\n",
    "        # Create the 2nd subplot - SLE\n",
    "        ax2 = fig.add_subplot(gs[1])\n",
    "        ax2.set_aspect('auto')  # Adjust the aspect ratio if needed\n",
    "        p2 = ax2.plot(lat,sle)\n",
    "        ax2.set_xlabel('Latitude, Longitude')\n",
    "        plt.xticks(ticks=t,labels=t2)\n",
    "        ax2.set_ylabel('SLE')\n",
    "        plt.xlim(lat.min(), lat.max())\n",
    "        plt.axvline(ds.lat.sel(time=mintime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(ds.lat.sel(time=maxtime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(sicedge_lat, color='orange', linestyle='--')\n",
    "        plt.axvline(miz_inner_lat[2], color='pink', linestyle='--')\n",
    "\n",
    "        #########################\n",
    "        # Create the third subplot - SIC\n",
    "        ax3 = fig.add_subplot(gs[2])\n",
    "        # Set the size of the second subplot\n",
    "        ax3.set_aspect('auto')  # Adjust the aspect ratio if needed\n",
    "        #p3 = ax3.plot(lat,flag)\n",
    "        p3 = ax3.plot(lat,sic)\n",
    "        p3 = ax3.plot(lat,sicflag_thresh)\n",
    "        p3 = ax3.plot(lat,sicflag_thresh_rolmed_1s+0.1)\n",
    "        p3 = ax3.plot(lat,sicflag_thresh_rolmed_2s+0.2)\n",
    "        ax3.set_xlabel('Latitude, Longitude')\n",
    "        plt.xticks(ticks=t,labels=t2)\n",
    "        ax3.set_ylabel('Flag')\n",
    "        plt.xlim(lat.min(), lat.max())\n",
    "        plt.axvline(ds.lat.sel(time=mintime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(ds.lat.sel(time=maxtime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(sicedge_lat, color='orange', linestyle='--')\n",
    "        plt.axvline(miz_inner_lat[2], color='pink', linestyle='--')\n",
    "\n",
    "        #########################\n",
    "        # Create the 4th subplot - peakiness\n",
    "        ax4 = fig.add_subplot(gs[3])\n",
    "        # Set the size of the second subplot\n",
    "        ax4.set_aspect('auto')  # Adjust the aspect ratio if needed\n",
    "        p4 = ax4.plot(lat,peakiness)#.rolling(time=41, center=True).mean())\n",
    "        ax4.set_xlabel('Latitude, Longitude')\n",
    "        plt.xticks(ticks=t,labels=t2)\n",
    "        ax4.set_ylabel('Peakiness')\n",
    "        plt.xlim(lat.min(), lat.max())\n",
    "        plt.axvline(ds.lat.sel(time=mintime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(ds.lat.sel(time=maxtime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(sicedge_lat, color='orange', linestyle='--')\n",
    "        plt.axvline(miz_inner_lat[2], color='pink', linestyle='--')\n",
    "\n",
    "        #########################\n",
    "        # Create the 5th subplot - product\n",
    "        ax5 = fig.add_subplot(gs[4])\n",
    "        # Set the size of the second subplot\n",
    "        ax5.set_aspect('auto')  # Adjust the aspect ratio if needed\n",
    "        p5 = ax5.plot(lat,SLE_peakiness_product)#swh.rolling(time=41, center=True).mean())#.rolling(time=41, center=True).mean())\n",
    "        p5s = ax5.plot(lat,SLE_peakiness_smoothed)#swh.rolling(time=41, center=True).mean())#.rolling(time=41, center=True).mean())\n",
    "        ax5.set_xlabel('Latitude, Longitude')\n",
    "        plt.xticks(ticks=t,labels=t2)\n",
    "        ax5.set_ylabel('SLE.peakiness product')\n",
    "        plt.xlim(lat.min(), lat.max())\n",
    "        plt.axvline(ds.lat.sel(time=mintime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(ds.lat.sel(time=maxtime, method='nearest'), color='red', linestyle='--')\n",
    "        plt.axvline(sicedge_lat, color='orange', linestyle='--')\n",
    "        plt.axvline(miz_inner_lat[2], color='pink', linestyle='--')\n",
    "\n",
    "        # save it as PDF...\n",
    "        plt.savefig('./waveform_output/v'+version+'/AltiKa_output_v'+version+'_'+(ds.attrs['first_meas_time'].replace(\" \", \"_\")).replace(\":\", \"_\")+'.pdf', format='pdf')\n",
    "        plt.close()\n",
    " \n",
    "    return None    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4339bd-a32e-4b57-a3da-3c982dde394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting /scratch/jk72/af1544/altika_data/2024/SRL_GPS_2PfP185_0222_20240903_143728_20240903_152745.CNES.nc... "
     ]
    }
   ],
   "source": [
    "directory = '/scratch/jk72/af1544/altika_data/'+yearstring+'/'\n",
    "outfile = './csv_output/'+yearstring+'_all_output_v'+version+'.csv'\n",
    "\n",
    "# set up static vars\n",
    "vars_to_keep=['waveforms_40hz', 'open_sea_ice_flag', 'time', 'meas_ind', 'wvf_ind', 'lat', 'lon', 'lat_40hz', 'lon_40hz', 'time_40hz', 'surface_type', 'swh_40hz', 'agc_40hz', 'agc_corr_40hz', 'mean_wave_period_t02', 'mean_wave_direction', 'peakiness_40hz', 'scaling_factor_40hz']\n",
    "range_index = np.arange(0, 128)\n",
    "\n",
    "# iterate over files in that directory\n",
    "for idx, filename in enumerate(sorted(os.listdir(directory))):\n",
    "    # select a specific file to play with\n",
    "    #if filename != \"SRL_GPS_2PfP133_0142_20190907_154500_20190907_163519.CNES.nc\":\n",
    "    #    continue\n",
    "\n",
    "    # how to select a range of files (for completing incomplete processing years)\n",
    "    #if idx <= 7060:\n",
    "    #    continue\n",
    "\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        log('Starting '+f+'... ', end='')        \n",
    "\n",
    "        # Open dataset...\n",
    "        try:\n",
    "            dataset=xr.open_dataset(f)\n",
    "        except ValueError as e:\n",
    "            # Handle the specific error (e.g., log it, print a warning)\n",
    "            log('Failed to open '+f+'... ', end='')\n",
    "            # Continue to the next iteration of the loop\n",
    "            continue\n",
    "        keepers=dataset\n",
    "        keepers = keepers.drop([var for var in dataset.variables if var not in vars_to_keep])\n",
    "        combined_keepers = keepers.stack(record=(\"time\", \"meas_ind\"))\n",
    "        basename=os.path.basename(f)\n",
    "\n",
    "\n",
    "        # Create the coordinates for the altika data\n",
    "        coords = {\n",
    "            'time': combined_keepers.time_40hz.data,\n",
    "            'lat': xr.DataArray(combined_keepers.lat_40hz.data, dims='time'),\n",
    "            'lon': xr.DataArray(combined_keepers.lon_40hz.data, dims='time'),\n",
    "        }\n",
    "\n",
    "        # Create the xarray dataset\n",
    "        ds = xr.Dataset(\n",
    "            {\n",
    "                'waveforms_40hz': (['range', 'time'], combined_keepers.waveforms_40hz.data),\n",
    "                'open_sea_ice_flag': (['time'], combined_keepers.open_sea_ice_flag.data),\n",
    "                'surface_type': (['time'], combined_keepers.surface_type.data),\n",
    "                'swh_40hz': (['time'], combined_keepers.swh_40hz.data),\n",
    "                'agc_40hz': (['time'], combined_keepers.agc_40hz.data),\n",
    "                'agc_corr_40hz': (['time'], combined_keepers.agc_corr_40hz.data),\n",
    "                'peakiness_40hz': (['time'], combined_keepers.peakiness_40hz.data),\n",
    "                'scaling_factor_40hz': (['time'], combined_keepers.scaling_factor_40hz.data)\n",
    "            },\n",
    "            coords=coords\n",
    "        )\n",
    "\n",
    "        dataset.close()\n",
    "        combined_keepers.close()\n",
    "        del coords\n",
    "        \n",
    "        # Add range as a dimension coordinate\n",
    "        ds = ds.assign_coords(range=range_index)\n",
    "        # somehow time has some gaps. The waveform is missing there too. Need to delete these...\n",
    "        ds = ds.where(~np.isnat(ds.time), np.nan)\n",
    "        ds = ds.dropna(dim='time')\n",
    "        ds.attrs['first_meas_time'] = keepers.first_meas_time\n",
    "\n",
    "        keepers.close()\n",
    "\n",
    "        # get the SLE\n",
    "        sle=wf2sle2d(ds.waveforms_40hz)\n",
    "        # plug it back into the xarray dataset\n",
    "        ds_with_sle=ds.assign(sle_40hz=sle)\n",
    "\n",
    "        # open the associated sic along track dataset...\n",
    "        # Use a regular expression to extract the date-time string \n",
    "        match = re.search(r'(\\d{8})_(\\d{6})', basename)\n",
    "        if match:\n",
    "            date_str, time_str = match.groups()\n",
    "        # Format the date and time strings\n",
    "        formatted_date = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "        formatted_time = f\"{time_str[:2]}_{time_str[2:4]}_{time_str[4:]}\"\n",
    "        sicname = f\"{formatted_date}_{formatted_time}\"\n",
    "        sicfullname = glob.glob('./sic_retrieval_output/v0_12/'+yearstring+'/Altika_alongtrack_sic_v0_12_'+sicname+'*.nc')\n",
    "        # open the SIC dataset...\n",
    "        try:\n",
    "            ds_sic = xr.open_dataset(sicfullname[0])\n",
    "        except IndexError:\n",
    "            log('Missing SIC file I think... Done.', end=\"\")    \n",
    "            continue\n",
    "\n",
    "        # Resample SIC to 40 Hz.\n",
    "        # I can reference it against lat.\n",
    "        # Create a new DataArray for interpolated sic data, initialised with NaNs\n",
    "        try: \n",
    "            indices = [find_nearest(ds_sic['lat'].values, val) for val in ds_with_sle['lat'].values]\n",
    "        except ValueError:\n",
    "            log('ValueError in the resampling SIC to 40 Hz block... Done.', end=\"\")    \n",
    "            continue\n",
    "        sic_highres = xr.DataArray(\n",
    "            [ds_sic['sic'].values[idx] for idx in indices],\n",
    "            dims=('time',),\n",
    "            coords={'time': ds_with_sle['time']})\n",
    "        # Also include the sic at +2 and -2 degrees of longitude.\n",
    "        sic_highres_p2 = xr.DataArray(\n",
    "            [ds_sic['sicp2'].values[idx] for idx in indices],\n",
    "            dims=('time',),\n",
    "            coords={'time': ds_with_sle['time']})\n",
    "        sic_highres_m2 = xr.DataArray(\n",
    "            [ds_sic['sicm2'].values[idx] for idx in indices],\n",
    "            dims=('time',),\n",
    "            coords={'time': ds_with_sle['time']})\n",
    "        # Add SIC to the dataset.\n",
    "        ds_with_sle['sic'] = sic_highres\n",
    "        ds_with_sle['sicp2'] = sic_highres_p2\n",
    "        ds_with_sle['sicm2'] = sic_highres_m2\n",
    "\n",
    "\n",
    "        ds_sic.close()\n",
    "        \n",
    "        #and plot/save!\n",
    "        multi_panel_plot_with_MIZ_retrieval(ds_with_sle, -80, -55, outfile, plotting)\n",
    "        ds.close()\n",
    "        ds_with_sle.close()\n",
    "        \n",
    "        log('Done.')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673cd90-3e5d-4520-a09d-9c047013a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771e871-ed97-4c99-8dee-33ab399473b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb76925-0069-4b4c-b22c-99ab548f0a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
